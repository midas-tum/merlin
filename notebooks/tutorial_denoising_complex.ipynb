{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Example 03: Denoising on complex-valued data with complex-valued operations**\n",
    "We first define the data pipelines to feed the data into training, validation and test set. The MNIST database is used for showcasing. Since MNIST are real-valued images, a phase is simulated and added to the images to generate a complex-valued input. A white Gaussian noise is simulated retrospectively and added to the data. The task of the network is to denoise the images with complex-valued operations. You can compare the different processing to the pure real-valued case (Example 01) and to handling the complex-valued data in 2 real-valued channels (Example 03).\n",
    "\n",
    "To enable GPU support in Google Colab, please go to `Edit -> Notebook settings` and select `GPU` as hardware accelerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# inspect the available GPU hardware\n",
    "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Database pipeline\n",
    "Here starts the main part of the script. First define the data pipelines (in the form of generator functions) for training, validation and test set. Retrospective noise simulation is performed inside the generator functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches to process: 1500\n",
      "Validation batches to process: 375\n",
      "Test samples to process: 10000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import tutorial\n",
    "import merlintf\n",
    "\n",
    "# initialize some parameters\n",
    "noise_level = 0.5  # simulated additive white Gaussian noise level\n",
    "\n",
    "# Data Generators (Data pipeline) for complex-valued data\n",
    "# training set\n",
    "training_generator = tutorial.datasets.ComplexDataGeneratorMNIST(batch_size=32, \n",
    "                                    noise_level=noise_level,\n",
    "                                    shuffle=True,\n",
    "                                    mode='train')\n",
    "\n",
    "# validation set\n",
    "validation_generator = tutorial.datasets.ComplexDataGeneratorMNIST(batch_size=32, \n",
    "                                    noise_level=noise_level,\n",
    "                                    shuffle=False,\n",
    "                                    mode='val')\n",
    "\n",
    "# test set\n",
    "# ideally testing should be performed on real noisy cases and not simulated ones\n",
    "test_generator = tutorial.datasets.ComplexDataGeneratorMNIST(batch_size=1,   \n",
    "                                    shuffle=False,\n",
    "                                    mode='test')\n",
    "\n",
    "print('Training batches to process:', len(training_generator))\n",
    "print('Validation batches to process:', len(validation_generator))\n",
    "print('Test samples to process:', len(test_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Model\n",
    "Define the CNN model with its corresponding inputs and outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3-layer convolutional neural network (CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"3layerCNNComplex\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "complex_conv2d (ComplexConv2 (None, 28, 28, 4)         80        \n",
      "_________________________________________________________________\n",
      "complex_conv2d_1 (ComplexCon (None, 28, 28, 4)         296       \n",
      "_________________________________________________________________\n",
      "complex_conv2d_2 (ComplexCon (None, 28, 28, 1)         74        \n",
      "=================================================================\n",
      "Total params: 450\n",
      "Trainable params: 450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Generate Model\n",
    "# Let's start with a 3-layer CNN\n",
    "input = tf.keras.Input(shape=(28, 28, 1), dtype='complex64')  # define input layer and its shape, complex-valued tensors\n",
    "activation = 'cReLU'  # select activation function: complex-valued ReLU\n",
    "# convolutional layer 1: Complex-valued convolution\n",
    "conv_out1 = merlintf.keras.layers.ComplexConv2D(filters=4,                # output channels, N_fout\n",
    "                                kernel_size=(3,3),              # kernel size along x and y\n",
    "                                strides=(1,1),                  # stride performed along x and y\n",
    "                                padding='SAME',                 # padding of input to adjust output size\n",
    "                                use_bias=True,                  # learn bias values for conv layer\n",
    "                                activation=activation)(input)   # apply activation function after conv operation\n",
    "# convolutional layer 2: Complex-valued convolution\n",
    "conv_out2 = merlintf.keras.layers.ComplexConv2D(filters=4,\n",
    "                                kernel_size=(3,3),\n",
    "                                strides=(1,1),\n",
    "                                padding='SAME',\n",
    "                                use_bias=True,\n",
    "                                activation=activation)(conv_out1)\n",
    "# convolutional layer 3: Complex-valued convolution\n",
    "output    = merlintf.keras.layers.ComplexConv2D(filters=1,\n",
    "                                kernel_size=(3,3),\n",
    "                                strides=(1,1),\n",
    "                                padding='SAME',\n",
    "                                use_bias=True,\n",
    "                                activation=activation)(conv_out2)\n",
    "\n",
    "# instantiate a keras functional model: combine layers into a model with specified inputs and outputs\n",
    "model = tf.keras.Model(input, output, name='3layerCNNComplex')\n",
    "\n",
    "# print model overview\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3-layer residual convolutional neural network (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Residual3layerCNNComplex\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 28, 28, 1)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "complex_conv2d (ComplexConv2D)  (None, 28, 28, 4)    80          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "complex_conv2d_1 (ComplexConv2D (None, 28, 28, 4)    296         complex_conv2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "complex_conv2d_2 (ComplexConv2D (None, 28, 28, 1)    74          complex_conv2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 28, 28, 1)    0           input_1[0][0]                    \n",
      "                                                                 complex_conv2d_2[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 450\n",
      "Trainable params: 450\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Generate Model\n",
    "# Let's start with a residual 3-layer CNN\n",
    "input = tf.keras.Input(shape=(28, 28, 1), dtype='complex64')  # define input layer and its shape, complex-valued tensors\n",
    "activation = 'cReLU'  # select activation function: complex-valued ReLU\n",
    "# convolutional layer 1: Complex-valued convolution\n",
    "conv_out1 = merlintf.keras.layers.ComplexConv2D(filters=4,                # output channels, N_fout\n",
    "                                kernel_size=(3,3),              # kernel size along x and y\n",
    "                                strides=(1,1),                  # stride performed along x and y\n",
    "                                padding='SAME',                 # padding of input to adjust output size\n",
    "                                use_bias=True,                  # learn bias values for conv layer\n",
    "                                activation=activation)(input)   # apply activation function after conv operation\n",
    "# convolutional layer 2: Complex-valued convolution\n",
    "conv_out2 = merlintf.keras.layers.ComplexConv2D(filters=4,\n",
    "                                kernel_size=(3,3),\n",
    "                                strides=(1,1),\n",
    "                                padding='SAME',\n",
    "                                use_bias=True,\n",
    "                                activation=activation)(conv_out1)\n",
    "# convolutional layer 3: Complex-valued convolution\n",
    "residual    = merlintf.keras.layers.ComplexConv2D(filters=1,\n",
    "                                kernel_size=(3,3),\n",
    "                                strides=(1,1),\n",
    "                                padding='SAME',\n",
    "                                use_bias=True,\n",
    "                                activation=activation)(conv_out2)\n",
    "# residual connection\n",
    "output = tf.keras.layers.Add()([input, residual])\n",
    "\n",
    "# instantiate a keras functional model: combine layers into a model with specified inputs and outputs\n",
    "model = tf.keras.Model(input, output, name='Residual3layerCNNComplex')\n",
    "\n",
    "# print model overview\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Build model\n",
    "Compile the model, assign an optimizer, loss function and validation metrics. Prepare some keras callbacks to monitor training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),      # used optimizer with chosen learning rate\n",
    "              loss='mse',                                                   # loss function (note: TF can perform real-valued mse loss on complex-valued input) \n",
    "              metrics=['mse', 'mae'])                                       # evaluation metrics (for training and validation set)\n",
    "\n",
    "# define callbacks to monitor model\n",
    "keras_callbacks = tutorial.get_callbacks(validation_generator, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Tensorboard\n",
    "Start the Tensorboard [optional] to monitor training progress and display validation outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start Tensorboard\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training\n",
    "Train the configured and compiled model. Monitor training progress with validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1500/1500 [==============================] - 15s 9ms/step - loss: 0.0457 - mse: 0.0410 - mae: 0.0726 - val_loss: 0.0342 - val_mse: 0.0317 - val_mae: 0.0531\n",
      "Epoch 2/3\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0338 - mse: 0.0315 - mae: 0.0521 - val_loss: 0.0331 - val_mse: 0.0310 - val_mae: 0.0505\n",
      "Epoch 3/3\n",
      "1500/1500 [==============================] - 12s 8ms/step - loss: 0.0332 - mse: 0.0312 - mae: 0.0505 - val_loss: 0.0327 - val_mse: 0.0308 - val_mae: 0.0499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ffa383713c8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model with training set and evaluate its performance with the validation set\n",
    "model.fit(training_generator,                       # training set\n",
    "          validation_data=validation_generator,     # validation set\n",
    "          epochs=3,                                 # number of epochs to train the model\n",
    "          callbacks=keras_callbacks)                # callbacks to monitor or control training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Testing\n",
    "Test the trained model to predict a denoised output and to display performance (metrics) on test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 26s 3ms/step - loss: 0.0311 - mse: 0.0301 - mae: 0.0454\n"
     ]
    }
   ],
   "source": [
    "# predict with trained model\n",
    "predicted_output = model.predict(test_generator)\n",
    "\n",
    "# evaluate trained model\n",
    "loss_metric_test = model.evaluate(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display the predicted output\n",
    "import matplotlib.pyplot as plt\n",
    "icase = 0  # display the first example\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(np.squeeze(np.abs(predicted_output[icase,])), cmap='gray')\n",
    "plt.title('Magnitude')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(np.squeeze(np.angle(predicted_output[icase,])))\n",
    "plt.title('Phase')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26c4402bb8476a31d8c429f22841c868a499a7e31d9c723639b1a5309d819501"
  },
  "kernelspec": {
   "display_name": "Python 3.6.11 64-bit ('optox_midas': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
